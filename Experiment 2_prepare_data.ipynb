{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ebf9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, product\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import tqdm\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import embeddings_functions\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d945f754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter-\n",
      "[nltk_data]     berl03@vse.cz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jupyter-\n",
      "[nltk_data]     berl03@vse.cz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter-\n",
      "[nltk_data]     berl03@vse.cz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jupyter-\n",
      "[nltk_data]     berl03@vse.cz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891284e7",
   "metadata": {},
   "source": [
    "## Yelp Dataset preparation\n",
    "- https://www.kaggle.com/datasets/mexwell/yelp-review-dataset\n",
    "- Remove interpunctions\n",
    "- Convert to lower case\n",
    "- Select just english texts\n",
    "- Remove stopwords\n",
    "- Remove unusual words\n",
    "- Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train,test])\n",
    "df_proc = df[[\"text\"]].reset_index()\n",
    "print(\"dataset was read\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[\\W\\d_]', ' ', text)  # Remove non-word characters, digits, and underscores\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df_proc['text'] = df_proc['text'].apply(preprocess_text)\n",
    "\n",
    "df_proc = df_proc.dropna()\n",
    "max_length = 30\n",
    "df_proc = df_proc[df_proc['text'].apply(len) >= max_length] # jinak nešel aplikovat language detection\n",
    "\n",
    "\n",
    "languages = []\n",
    "for text in df_proc['text']:\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        language = 'unknown'\n",
    "    languages.append(language)\n",
    "df_proc['language'] = languages\n",
    "\n",
    "df_proc = df_proc[df_proc['language'] == 'en']\n",
    "\n",
    "# Tokenize text\n",
    "df_proc['text_tok'] = df_proc['text'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_proc['text_tok'] = df_proc['text_tok'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# Flatten list of tokens and create a frequency distribution\n",
    "all_tokens = [word for tokens in df_proc['text_tok'] for word in tokens]\n",
    "freq_dist = Counter(all_tokens)\n",
    "\n",
    "# Determine thresholds for what is considered \"unusual\"\n",
    "min_freq = 5\n",
    "max_freq = 0.1 * len(df_proc)\n",
    "\n",
    "# Filter out unusual words\n",
    "df_proc['text_tok'] = df_proc['text_tok'].apply(lambda tokens: [word for word in tokens if min_freq <= freq_dist[word] <= max_freq])\n",
    "\n",
    "df_proc = df_proc.drop_duplicates([\"text\"])\n",
    "\n",
    "print(len(df_proc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uložení dataframe do Parquet\n",
    "df_proc.to_parquet('df_proc.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e684a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.read_parquet(\"df_proc.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd5e410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>text_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dr  goldberg offers everything i look for in a...</td>\n",
       "      <td>en</td>\n",
       "      <td>[dr, goldberg, offers, everything, look, gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>unfortunately  the frustration of being dr  go...</td>\n",
       "      <td>en</td>\n",
       "      <td>[unfortunately, frustration, dr, goldberg, pat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text language  \\\n",
       "0      0  dr  goldberg offers everything i look for in a...       en   \n",
       "1      1  unfortunately  the frustration of being dr  go...       en   \n",
       "\n",
       "                                            text_tok  \n",
       "0  [dr, goldberg, offers, everything, look, gener...  \n",
       "1  [unfortunately, frustration, dr, goldberg, pat...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d87bc",
   "metadata": {},
   "source": [
    "## All unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e826c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34701335\n",
      "66288\n"
     ]
    }
   ],
   "source": [
    "flattened_list = list(chain(*df_proc['text_tok'].tolist()))\n",
    "print(len(flattened_list))\n",
    "unique_list = list(set(flattened_list))\n",
    "print(len(unique_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046194d",
   "metadata": {},
   "source": [
    "## MEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a63a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n",
      "731\n"
     ]
    }
   ],
   "source": [
    "men = pd.read_csv(\"men.txt\", sep=\"\\t\")\n",
    "men[\"score\"] = men[\"score\"]/100\n",
    "men[\"word1\"] = men[\"word1\"].astype(str)\n",
    "\n",
    "words_men = list(set(list(men[\"word1\"].values)+list(men[\"word2\"].values)))\n",
    "print(len(words_men))\n",
    "\n",
    "words_men = [word for word in words_men if word in unique_list]\n",
    "print(len(words_men))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65480af9",
   "metadata": {},
   "source": [
    "## Choose random words + words from MEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8337105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random_words_list = random.sample(unique_list, 3000)\n",
    "\n",
    "df_random = pd.DataFrame(random_words_list,columns = [\"word\"])\n",
    "df_random[\"is_men\"] = 0\n",
    "df_men = pd.DataFrame(words_men,columns = [\"word\"])\n",
    "df_men[\"is_men\"] = 1\n",
    "df_tot = pd.concat([df_random,df_men])\n",
    "\n",
    "df_tot.to_csv(\"random_words_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950bd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = words_men+random_words_list\n",
    "word_pairs = list(combinations(total_words, 2))\n",
    "df_word_pairs = pd.DataFrame(word_pairs, columns=['First', 'Last'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6942e2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flood</td>\n",
       "      <td>pod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flood</td>\n",
       "      <td>neon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flood</td>\n",
       "      <td>chapel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flood</td>\n",
       "      <td>vintage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flood</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958310</th>\n",
       "      <td>anybody</td>\n",
       "      <td>cabane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958311</th>\n",
       "      <td>anybody</td>\n",
       "      <td>bofa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958312</th>\n",
       "      <td>unreasonable</td>\n",
       "      <td>cabane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958313</th>\n",
       "      <td>unreasonable</td>\n",
       "      <td>bofa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958314</th>\n",
       "      <td>cabane</td>\n",
       "      <td>bofa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6958315 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                First     Last\n",
       "0               flood      pod\n",
       "1               flood     neon\n",
       "2               flood   chapel\n",
       "3               flood  vintage\n",
       "4               flood      red\n",
       "...               ...      ...\n",
       "6958310       anybody   cabane\n",
       "6958311       anybody     bofa\n",
       "6958312  unreasonable   cabane\n",
       "6958313  unreasonable     bofa\n",
       "6958314        cabane     bofa\n",
       "\n",
       "[6958315 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128b7a1",
   "metadata": {},
   "source": [
    "## Derive Dataset Characteristics Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9038dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_proc[\"text_tok\"].to_list()\n",
    "total_word_count = df_proc['text'].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f4c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_and_co_occurrence(window_size, sentences, df_word_pairs, total_word_count):\n",
    "    \n",
    "    def calc_co_occurrence_matrix(sentences, window_size):\n",
    "        frekvence_slov = Counter()\n",
    "        d = defaultdict(int)\n",
    "        vocab = set()  \n",
    "        for text in tqdm(sentences):\n",
    "            frekvence_slov.update(text)\n",
    "            for i, token in enumerate(text):\n",
    "                vocab.add(token)\n",
    "                next_tokens = text[i+1 : i+1+window_size]\n",
    "                for next_token in next_tokens:\n",
    "                    key = tuple(sorted([next_token, token]))\n",
    "                    d[key] += 1             \n",
    "        vocab = sorted(vocab)\n",
    "        vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "        co_occurrence_matrix = np.zeros((len(vocab), len(vocab)), dtype=int)\n",
    "\n",
    "        for (token1, token2), value in tqdm(d.items()):\n",
    "            index1 = vocab_index[token1]\n",
    "            index2 = vocab_index[token2]\n",
    "            co_occurrence_matrix[index1, index2] = value\n",
    "            co_occurrence_matrix[index2, index1] = value\n",
    "        return co_occurrence_matrix, vocab_index, frekvence_slov\n",
    "        \n",
    "    def calc_columns(df_word_pairs, vocab_index, co_occurrence_matrix, frekvence_slov, window_size, total_word_count):\n",
    "        frequencies = []\n",
    "        frequencies_first = []\n",
    "        frequencies_last = []\n",
    "        correlations = []\n",
    "        \n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, row in tqdm(df_word_pairs.iterrows(), total=df_word_pairs.shape[0]):\n",
    "            word1, word2 = row[\"First\"], row[\"Last\"]\n",
    "            index_w1, index_w2 = vocab_index.get(word1), vocab_index.get(word2)\n",
    "            \n",
    "            if index_w1 is not None and index_w2 is not None:\n",
    "                valid_indices.append(i)\n",
    "                frequency = co_occurrence_matrix[index_w1, index_w2]\n",
    "                frequencies.append(frequency)\n",
    "                frequencies_first.append(frekvence_slov.get(word1, 0))\n",
    "                frequencies_last.append(frekvence_slov.get(word2, 0))\n",
    "                correlation = np.corrcoef(co_occurrence_matrix[index_w1], co_occurrence_matrix[index_w2])[0, 1]\n",
    "                correlations.append(correlation)\n",
    "            else:\n",
    "                # Skip this pair if either word is not in the vocabulary\n",
    "                continue\n",
    "            \n",
    "        # Now use valid_indices to filter df_word_pairs and assign values\n",
    "        df_word_pairs = df_word_pairs.iloc[valid_indices].copy()\n",
    "\n",
    "        if 'frequency_w1' not in df_word_pairs.columns: # only once - not depended on window\n",
    "            df_word_pairs[\"no_words_corpus\"] = total_word_count\n",
    "            df_word_pairs['frequency_w1'] = frequencies_first\n",
    "            df_word_pairs['frequency_w2'] = frequencies_last\n",
    "            df_word_pairs[\"tf_rel_word1\"] = df_word_pairs[\"frequency_w1\"] / df_word_pairs[\"no_words_corpus\"] * 100\n",
    "            df_word_pairs[\"tf_rel_word2\"] = df_word_pairs[\"frequency_w2\"] / df_word_pairs[\"no_words_corpus\"] * 100\n",
    "            df_word_pairs['word1_length'] = df_word_pairs['First'].apply(len)\n",
    "            df_word_pairs['word2_length'] = df_word_pairs['Last'].apply(len)\n",
    "\n",
    "        if \"fraq_w1_w2\" not in df_word_pairs.columns:\n",
    "            df_word_pairs[\"fraq_w1_w2\"] = df_word_pairs[\"frequency_w1\"] / df_word_pairs[\"frequency_w2\"]\n",
    "            \n",
    "        df_word_pairs['frequency_of_cooc_w_'+str(window_size)] = frequencies\n",
    "        df_word_pairs[\"corr_w_\" + str(window_size)] = correlations\n",
    "        df_word_pairs[\"freq_cooc_mult_fraq_w1_w2_w_\"+str(window_size)] = df_word_pairs['frequency_of_cooc_w_'+str(window_size)] * df_word_pairs[\"fraq_w1_w2\"]\n",
    "\n",
    "        return df_word_pairs\n",
    "\n",
    "    co_occurrence_matrix, vocab_index, frekvence_slov = calc_co_occurrence_matrix(sentences, window_size)\n",
    "    df_word_pairs = calc_columns(df_word_pairs, vocab_index, co_occurrence_matrix, frekvence_slov, window_size, total_word_count)\n",
    "    return df_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2433e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691190/691190 [01:23<00:00, 8310.13it/s] \n",
      "100%|██████████| 21502623/21502623 [00:15<00:00, 1402932.79it/s]\n",
      "100%|██████████| 6958315/6958315 [27:00<00:00, 4293.21it/s]\n",
      "100%|██████████| 691190/691190 [01:53<00:00, 6104.51it/s]\n",
      "100%|██████████| 30080963/30080963 [00:20<00:00, 1480443.11it/s]\n",
      "100%|██████████| 6958315/6958315 [27:20<00:00, 4242.75it/s]\n",
      "100%|██████████| 691190/691190 [02:43<00:00, 4218.68it/s]\n",
      "100%|██████████| 39881412/39881412 [00:24<00:00, 1596818.10it/s]\n",
      "100%|██████████| 6958315/6958315 [27:35<00:00, 4202.48it/s]\n"
     ]
    }
   ],
   "source": [
    "for window_size in [3,5,8]:\n",
    "    df_word_pairs = build_vocab_and_co_occurrence(window_size, sentences,df_word_pairs,total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7672487d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>no_words_corpus</th>\n",
       "      <th>frequency_w1</th>\n",
       "      <th>frequency_w2</th>\n",
       "      <th>tf_rel_word1</th>\n",
       "      <th>tf_rel_word2</th>\n",
       "      <th>word1_length</th>\n",
       "      <th>word2_length</th>\n",
       "      <th>fraq_w1_w2</th>\n",
       "      <th>frequency_of_cooc_w_3</th>\n",
       "      <th>corr_w_3</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_3</th>\n",
       "      <th>frequency_of_cooc_w_5</th>\n",
       "      <th>corr_w_5</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_5</th>\n",
       "      <th>frequency_of_cooc_w_8</th>\n",
       "      <th>corr_w_8</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flood</td>\n",
       "      <td>pod</td>\n",
       "      <td>509377579</td>\n",
       "      <td>228</td>\n",
       "      <td>305</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.747541</td>\n",
       "      <td>0</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.393259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flood</td>\n",
       "      <td>neon</td>\n",
       "      <td>509377579</td>\n",
       "      <td>228</td>\n",
       "      <td>1034</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.220503</td>\n",
       "      <td>0</td>\n",
       "      <td>0.216630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.323404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.417006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flood</td>\n",
       "      <td>chapel</td>\n",
       "      <td>509377579</td>\n",
       "      <td>228</td>\n",
       "      <td>624</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.262472</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flood</td>\n",
       "      <td>vintage</td>\n",
       "      <td>509377579</td>\n",
       "      <td>228</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.127803</td>\n",
       "      <td>0</td>\n",
       "      <td>0.261986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.347716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405814</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flood</td>\n",
       "      <td>red</td>\n",
       "      <td>509377579</td>\n",
       "      <td>228</td>\n",
       "      <td>26879</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.380737</td>\n",
       "      <td>0.008482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958310</th>\n",
       "      <td>anybody</td>\n",
       "      <td>cabane</td>\n",
       "      <td>509377579</td>\n",
       "      <td>1435</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>119.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.144840</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958311</th>\n",
       "      <td>anybody</td>\n",
       "      <td>bofa</td>\n",
       "      <td>509377579</td>\n",
       "      <td>1435</td>\n",
       "      <td>70</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.220021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307855</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958312</th>\n",
       "      <td>unreasonable</td>\n",
       "      <td>cabane</td>\n",
       "      <td>509377579</td>\n",
       "      <td>525</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958313</th>\n",
       "      <td>unreasonable</td>\n",
       "      <td>bofa</td>\n",
       "      <td>509377579</td>\n",
       "      <td>525</td>\n",
       "      <td>70</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307578</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958314</th>\n",
       "      <td>cabane</td>\n",
       "      <td>bofa</td>\n",
       "      <td>509377579</td>\n",
       "      <td>12</td>\n",
       "      <td>70</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052537</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6958315 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                First     Last  no_words_corpus  frequency_w1  frequency_w2  \\\n",
       "0               flood      pod        509377579           228           305   \n",
       "1               flood     neon        509377579           228          1034   \n",
       "2               flood   chapel        509377579           228           624   \n",
       "3               flood  vintage        509377579           228          1784   \n",
       "4               flood      red        509377579           228         26879   \n",
       "...               ...      ...              ...           ...           ...   \n",
       "6958310       anybody   cabane        509377579          1435            12   \n",
       "6958311       anybody     bofa        509377579          1435            70   \n",
       "6958312  unreasonable   cabane        509377579           525            12   \n",
       "6958313  unreasonable     bofa        509377579           525            70   \n",
       "6958314        cabane     bofa        509377579            12            70   \n",
       "\n",
       "         tf_rel_word1  tf_rel_word2  word1_length  word2_length  fraq_w1_w2  \\\n",
       "0            0.000045      0.000060             5             3    0.747541   \n",
       "1            0.000045      0.000203             5             4    0.220503   \n",
       "2            0.000045      0.000123             5             6    0.365385   \n",
       "3            0.000045      0.000350             5             7    0.127803   \n",
       "4            0.000045      0.005277             5             3    0.008482   \n",
       "...               ...           ...           ...           ...         ...   \n",
       "6958310      0.000282      0.000002             7             6  119.583333   \n",
       "6958311      0.000282      0.000014             7             4   20.500000   \n",
       "6958312      0.000103      0.000002            12             6   43.750000   \n",
       "6958313      0.000103      0.000014            12             4    7.500000   \n",
       "6958314      0.000002      0.000014             6             4    0.171429   \n",
       "\n",
       "         frequency_of_cooc_w_3  corr_w_3  freq_cooc_mult_fraq_w1_w2_w_3  \\\n",
       "0                            0  0.279081                            0.0   \n",
       "1                            0  0.216630                            0.0   \n",
       "2                            0  0.142466                            0.0   \n",
       "3                            0  0.261986                            0.0   \n",
       "4                            0  0.181231                            0.0   \n",
       "...                        ...       ...                            ...   \n",
       "6958310                      0  0.058877                            0.0   \n",
       "6958311                      0  0.174527                            0.0   \n",
       "6958312                      0  0.052785                            0.0   \n",
       "6958313                      0  0.208968                            0.0   \n",
       "6958314                      0  0.018777                            0.0   \n",
       "\n",
       "         frequency_of_cooc_w_5  corr_w_5  freq_cooc_mult_fraq_w1_w2_w_5  \\\n",
       "0                            0  0.332584                            0.0   \n",
       "1                            0  0.323404                            0.0   \n",
       "2                            0  0.195536                            0.0   \n",
       "3                            0  0.347716                            0.0   \n",
       "4                            0  0.283348                            0.0   \n",
       "...                        ...       ...                            ...   \n",
       "6958310                      0  0.084616                            0.0   \n",
       "6958311                      0  0.220021                            0.0   \n",
       "6958312                      0  0.088481                            0.0   \n",
       "6958313                      0  0.252200                            0.0   \n",
       "6958314                      0  0.023756                            0.0   \n",
       "\n",
       "         frequency_of_cooc_w_8  corr_w_8  freq_cooc_mult_fraq_w1_w2_w_8  \n",
       "0                            0  0.393259                       0.000000  \n",
       "1                            0  0.417006                       0.000000  \n",
       "2                            0  0.262472                       0.000000  \n",
       "3                            0  0.405814                       0.000000  \n",
       "4                            1  0.380737                       0.008482  \n",
       "...                        ...       ...                            ...  \n",
       "6958310                      0  0.144840                       0.000000  \n",
       "6958311                      0  0.307855                       0.000000  \n",
       "6958312                      0  0.135014                       0.000000  \n",
       "6958313                      0  0.307578                       0.000000  \n",
       "6958314                      0  0.052537                       0.000000  \n",
       "\n",
       "[6958315 rows x 19 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c70ced38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pairs.to_csv(\"final_df_datasets_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pairs=pd.read_csv(\"final_df_datasets_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312177bb",
   "metadata": {},
   "source": [
    "## Lexical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd3db5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = df_word_pairs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e667d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res[\"first_second\"] = final_res[\"First\"] + \"_\" + final_res[\"Last\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "889d2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "men[\"first_second\"] = men[\"word1\"] + \"_\" + men[\"word2\"]\n",
    "f1 = men.merge(final_res, on = [\"first_second\"], how = \"inner\")\n",
    "men[\"first_second\"] = men[\"word2\"] + \"_\" + men[\"word1\"]\n",
    "f2 = men.merge(final_res, on = [\"first_second\"], how = \"inner\")\n",
    "fin = pd.concat([f1,f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769393ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>first_second</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>no_words_corpus</th>\n",
       "      <th>frequency_w1</th>\n",
       "      <th>frequency_w2</th>\n",
       "      <th>...</th>\n",
       "      <th>fraq_w1_w2</th>\n",
       "      <th>frequency_of_cooc_w_3</th>\n",
       "      <th>corr_w_3</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_3</th>\n",
       "      <th>frequency_of_cooc_w_5</th>\n",
       "      <th>corr_w_5</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_5</th>\n",
       "      <th>frequency_of_cooc_w_8</th>\n",
       "      <th>corr_w_8</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>river</td>\n",
       "      <td>water</td>\n",
       "      <td>0.49</td>\n",
       "      <td>river_water</td>\n",
       "      <td>1639782</td>\n",
       "      <td>river</td>\n",
       "      <td>water</td>\n",
       "      <td>509377579</td>\n",
       "      <td>1885</td>\n",
       "      <td>42608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044241</td>\n",
       "      <td>83</td>\n",
       "      <td>0.189442</td>\n",
       "      <td>3.671963</td>\n",
       "      <td>136</td>\n",
       "      <td>0.295877</td>\n",
       "      <td>6.016710</td>\n",
       "      <td>207</td>\n",
       "      <td>0.399837</td>\n",
       "      <td>9.157787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rain</td>\n",
       "      <td>storm</td>\n",
       "      <td>0.49</td>\n",
       "      <td>rain_storm</td>\n",
       "      <td>1122299</td>\n",
       "      <td>rain</td>\n",
       "      <td>storm</td>\n",
       "      <td>509377579</td>\n",
       "      <td>1675</td>\n",
       "      <td>666</td>\n",
       "      <td>...</td>\n",
       "      <td>2.515015</td>\n",
       "      <td>35</td>\n",
       "      <td>0.494082</td>\n",
       "      <td>88.025526</td>\n",
       "      <td>41</td>\n",
       "      <td>0.622359</td>\n",
       "      <td>103.115616</td>\n",
       "      <td>47</td>\n",
       "      <td>0.734905</td>\n",
       "      <td>118.205706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>kittens</td>\n",
       "      <td>0.49</td>\n",
       "      <td>cat_kittens</td>\n",
       "      <td>1348876</td>\n",
       "      <td>cat</td>\n",
       "      <td>kittens</td>\n",
       "      <td>509377579</td>\n",
       "      <td>2711</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>28.840426</td>\n",
       "      <td>6</td>\n",
       "      <td>0.423349</td>\n",
       "      <td>173.042553</td>\n",
       "      <td>11</td>\n",
       "      <td>0.508188</td>\n",
       "      <td>317.244681</td>\n",
       "      <td>15</td>\n",
       "      <td>0.568514</td>\n",
       "      <td>432.606383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat</td>\n",
       "      <td>feline</td>\n",
       "      <td>0.48</td>\n",
       "      <td>cat_feline</td>\n",
       "      <td>1348828</td>\n",
       "      <td>cat</td>\n",
       "      <td>feline</td>\n",
       "      <td>509377579</td>\n",
       "      <td>2711</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>58.934783</td>\n",
       "      <td>2</td>\n",
       "      <td>0.306128</td>\n",
       "      <td>117.869565</td>\n",
       "      <td>7</td>\n",
       "      <td>0.440324</td>\n",
       "      <td>412.543478</td>\n",
       "      <td>10</td>\n",
       "      <td>0.524349</td>\n",
       "      <td>589.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beach</td>\n",
       "      <td>sand</td>\n",
       "      <td>0.48</td>\n",
       "      <td>beach_sand</td>\n",
       "      <td>1475391</td>\n",
       "      <td>beach</td>\n",
       "      <td>sand</td>\n",
       "      <td>509377579</td>\n",
       "      <td>2996</td>\n",
       "      <td>1247</td>\n",
       "      <td>...</td>\n",
       "      <td>2.402566</td>\n",
       "      <td>56</td>\n",
       "      <td>0.403957</td>\n",
       "      <td>134.543705</td>\n",
       "      <td>79</td>\n",
       "      <td>0.551711</td>\n",
       "      <td>189.802727</td>\n",
       "      <td>98</td>\n",
       "      <td>0.662852</td>\n",
       "      <td>235.451484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>cheetah</td>\n",
       "      <td>phone</td>\n",
       "      <td>0.03</td>\n",
       "      <td>phone_cheetah</td>\n",
       "      <td>1649761</td>\n",
       "      <td>phone</td>\n",
       "      <td>cheetah</td>\n",
       "      <td>509377579</td>\n",
       "      <td>24611</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>665.162162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.221936</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>jellyfish</td>\n",
       "      <td>rally</td>\n",
       "      <td>0.02</td>\n",
       "      <td>rally_jellyfish</td>\n",
       "      <td>2284209</td>\n",
       "      <td>rally</td>\n",
       "      <td>jellyfish</td>\n",
       "      <td>509377579</td>\n",
       "      <td>123</td>\n",
       "      <td>132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>military</td>\n",
       "      <td>tomato</td>\n",
       "      <td>0.02</td>\n",
       "      <td>tomato_military</td>\n",
       "      <td>762213</td>\n",
       "      <td>tomato</td>\n",
       "      <td>military</td>\n",
       "      <td>509377579</td>\n",
       "      <td>11952</td>\n",
       "      <td>1046</td>\n",
       "      <td>...</td>\n",
       "      <td>11.426386</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195487</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>festival</td>\n",
       "      <td>whiskers</td>\n",
       "      <td>0.01</td>\n",
       "      <td>whiskers_festival</td>\n",
       "      <td>1139213</td>\n",
       "      <td>whiskers</td>\n",
       "      <td>festival</td>\n",
       "      <td>509377579</td>\n",
       "      <td>18</td>\n",
       "      <td>1734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170930</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>bakery</td>\n",
       "      <td>zebra</td>\n",
       "      <td>0.00</td>\n",
       "      <td>zebra_bakery</td>\n",
       "      <td>411775</td>\n",
       "      <td>zebra</td>\n",
       "      <td>bakery</td>\n",
       "      <td>509377579</td>\n",
       "      <td>71</td>\n",
       "      <td>4976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.201005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2976 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  score       first_second  Unnamed: 0     First  \\\n",
       "0         river     water   0.49        river_water     1639782     river   \n",
       "1          rain     storm   0.49         rain_storm     1122299      rain   \n",
       "2           cat   kittens   0.49        cat_kittens     1348876       cat   \n",
       "3           cat    feline   0.48         cat_feline     1348828       cat   \n",
       "4         beach      sand   0.48         beach_sand     1475391     beach   \n",
       "...         ...       ...    ...                ...         ...       ...   \n",
       "1486    cheetah     phone   0.03      phone_cheetah     1649761     phone   \n",
       "1487  jellyfish     rally   0.02    rally_jellyfish     2284209     rally   \n",
       "1488   military    tomato   0.02    tomato_military      762213    tomato   \n",
       "1489   festival  whiskers   0.01  whiskers_festival     1139213  whiskers   \n",
       "1490     bakery     zebra   0.00       zebra_bakery      411775     zebra   \n",
       "\n",
       "           Last  no_words_corpus  frequency_w1  frequency_w2  ...  fraq_w1_w2  \\\n",
       "0         water        509377579          1885         42608  ...    0.044241   \n",
       "1         storm        509377579          1675           666  ...    2.515015   \n",
       "2       kittens        509377579          2711            94  ...   28.840426   \n",
       "3        feline        509377579          2711            46  ...   58.934783   \n",
       "4          sand        509377579          2996          1247  ...    2.402566   \n",
       "...         ...              ...           ...           ...  ...         ...   \n",
       "1486    cheetah        509377579         24611            37  ...  665.162162   \n",
       "1487  jellyfish        509377579           123           132  ...    0.931818   \n",
       "1488   military        509377579         11952          1046  ...   11.426386   \n",
       "1489   festival        509377579            18          1734  ...    0.010381   \n",
       "1490     bakery        509377579            71          4976  ...    0.014268   \n",
       "\n",
       "      frequency_of_cooc_w_3  corr_w_3  freq_cooc_mult_fraq_w1_w2_w_3  \\\n",
       "0                        83  0.189442                       3.671963   \n",
       "1                        35  0.494082                      88.025526   \n",
       "2                         6  0.423349                     173.042553   \n",
       "3                         2  0.306128                     117.869565   \n",
       "4                        56  0.403957                     134.543705   \n",
       "...                     ...       ...                            ...   \n",
       "1486                      0  0.135797                       0.000000   \n",
       "1487                      0  0.137634                       0.000000   \n",
       "1488                      0  0.060476                       0.000000   \n",
       "1489                      0  0.113956                       0.000000   \n",
       "1490                      0  0.140158                       0.000000   \n",
       "\n",
       "      frequency_of_cooc_w_5  corr_w_5  freq_cooc_mult_fraq_w1_w2_w_5  \\\n",
       "0                       136  0.295877                       6.016710   \n",
       "1                        41  0.622359                     103.115616   \n",
       "2                        11  0.508188                     317.244681   \n",
       "3                         7  0.440324                     412.543478   \n",
       "4                        79  0.551711                     189.802727   \n",
       "...                     ...       ...                            ...   \n",
       "1486                      0  0.181042                       0.000000   \n",
       "1487                      0  0.176777                       0.000000   \n",
       "1488                      0  0.119615                       0.000000   \n",
       "1489                      0  0.134689                       0.000000   \n",
       "1490                      0  0.201005                       0.000000   \n",
       "\n",
       "      frequency_of_cooc_w_8  corr_w_8  freq_cooc_mult_fraq_w1_w2_w_8  \n",
       "0                       207  0.399837                       9.157787  \n",
       "1                        47  0.734905                     118.205706  \n",
       "2                        15  0.568514                     432.606383  \n",
       "3                        10  0.524349                     589.347826  \n",
       "4                        98  0.662852                     235.451484  \n",
       "...                     ...       ...                            ...  \n",
       "1486                      0  0.221936                       0.000000  \n",
       "1487                      0  0.244290                       0.000000  \n",
       "1488                      0  0.195487                       0.000000  \n",
       "1489                      0  0.170930                       0.000000  \n",
       "1490                      0  0.273046                       0.000000  \n",
       "\n",
       "[2976 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e9b866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_properties(word):\n",
    "    synonyms = set()\n",
    "    antonyms = set()\n",
    "    hyponyms = set()\n",
    "    hypernyms = set()\n",
    "    derivations = set()\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.antonyms()[0].name())\n",
    "            derivations.update(deriv.name() for deriv in lemma.derivationally_related_forms())\n",
    "        \n",
    "        hyponyms.update(lemma.name() for hyponym in syn.hyponyms() for lemma in hyponym.lemmas())\n",
    "        hypernyms.update(lemma.name() for hypernym in syn.hypernyms() for lemma in hypernym.lemmas())\n",
    "    \n",
    "    return synonyms, antonyms, hyponyms, hypernyms, derivations, len(synsets)\n",
    "\n",
    "def get_pos_tags(word):\n",
    "    doc = nlp(word)\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "def get_dependencies(word):\n",
    "    doc = nlp(word)\n",
    "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "\n",
    "def get_dependency_types(word):\n",
    "    doc = nlp(word)\n",
    "    return [token.dep_ for token in doc]\n",
    "\n",
    "def get_head_words(word):\n",
    "    doc = nlp(word)\n",
    "    return [token.head.text for token in doc]\n",
    "\n",
    "def get_dependency_count(word):\n",
    "    doc = nlp(word)\n",
    "    return len([token.dep_ for token in doc])\n",
    "\n",
    "def get_lemma(word):\n",
    "    doc = nlp(word)\n",
    "    return [token.lemma_ for token in doc][0]  \n",
    "\n",
    "def get_prefixes(word, n=3):\n",
    "    return [word[:i] for i in range(1, n+1)]\n",
    "\n",
    "def get_suffixes(word, n=3):\n",
    "    return [word[-i:] for i in range(1, n+1)]\n",
    "\n",
    "def process_single_pair(row):\n",
    "    first_word = row['word1']\n",
    "    second_word = row['word2']\n",
    "    \n",
    "    syn1, ant1, hypo1, hyper1, deriv1, poly1 = get_lexical_properties(first_word)\n",
    "    syn2, ant2, hypo2, hyper2, deriv2, poly2 = get_lexical_properties(second_word)\n",
    "\n",
    "\n",
    "    syn_match = len(syn1.intersection(syn2))\n",
    "    ant_match = len(ant1.intersection(ant2))\n",
    "\n",
    "    hypo_match = len(hypo1.intersection(hypo2))\n",
    "    hyper_match = len(hyper1.intersection(hyper2))\n",
    "\n",
    "\n",
    "    deriv_match = len(deriv1.intersection(deriv2))\n",
    "\n",
    "    # POS tags\n",
    "    pos1 = get_pos_tags(first_word)\n",
    "    pos2 = get_pos_tags(second_word)\n",
    "    common_pos_tags = len(set(pos1).intersection(set(pos2)))\n",
    "    same_pos_tags = pos1 == pos2\n",
    "\n",
    "    # Dependencies\n",
    "    dep1 = get_dependencies(first_word)\n",
    "    dep2 = get_dependencies(second_word)\n",
    "    dep_types1 = get_dependency_types(first_word)\n",
    "    dep_types2 = get_dependency_types(second_word)\n",
    "    head_words1 = get_head_words(first_word)\n",
    "    head_words2 = get_head_words(second_word)\n",
    "    dep_count1 = get_dependency_count(first_word)\n",
    "    dep_count2 = get_dependency_count(second_word)\n",
    "    common_dep_types = len(set(dep_types1).intersection(set(dep_types2)))\n",
    "    common_head_words = len(set(head_words1).intersection(set(head_words2)))\n",
    "    dep_count_diff = abs(dep_count1 - dep_count2)\n",
    "\n",
    "    # Lemmas\n",
    "    lemma1 = get_lemma(first_word)\n",
    "    lemma2 = get_lemma(second_word)\n",
    "    common_lemma = lemma1 == lemma2\n",
    "\n",
    "    # Prefixes and Suffixes\n",
    "    prefixes1 = set(get_prefixes(first_word))\n",
    "    prefixes2 = set(get_prefixes(second_word))\n",
    "    suffixes1 = set(get_suffixes(first_word))\n",
    "    suffixes2 = set(get_suffixes(second_word))\n",
    "    common_prefixes = len(prefixes1.intersection(prefixes2))\n",
    "    common_suffixes = len(suffixes1.intersection(suffixes2))\n",
    "\n",
    "\n",
    "    result = {\n",
    "        'syn_match': syn_match,\n",
    "        'syn_match_perc': syn_match / max(len(syn1), len(syn2)) if max(len(syn1), len(syn2)) != 0 else 0,\n",
    "        'syn1_count': len(syn1),\n",
    "        'syn2_count': len(syn2),\n",
    "        'are_synonyms': second_word in syn1,\n",
    "        'fraq_syn1_syn2': len(syn1) / len(syn2) if len(syn2) != 0 else 0,\n",
    "        'syn_match_mult_fraq_syn': syn_match * (len(syn1) / len(syn2) if len(syn2) != 0 else 0),\n",
    "\n",
    "        'ant_match': ant_match,\n",
    "        'ant_match_perc': ant_match / max(len(ant1), len(ant2)) if max(len(ant1), len(ant2)) != 0 else 0,\n",
    "        'ant1_count': len(ant1),\n",
    "        'ant2_count': len(ant2),\n",
    "        'are_antonyms': second_word in ant1,\n",
    "        'fraq_ant1_ant2': len(ant1) / len(ant2) if len(ant2) != 0 else 0,\n",
    "        'ant_match_mult_fraq_ant': ant_match * (len(ant1) / len(ant2) if len(ant2) != 0 else 0),\n",
    "\n",
    "        'hypo_match': hypo_match,\n",
    "        'hypo_match_perc': hypo_match / max(len(hypo1), len(hypo2)) if max(len(hypo1), len(hypo2)) != 0 else 0,\n",
    "        'hypo1_count': len(hypo1),\n",
    "        'hypo2_count': len(hypo2),\n",
    "        'is_hyponym': bool(hypo1.intersection({second_word})),\n",
    "        'fraq_hypo1_hypo2': len(hypo1) / len(hypo2) if len(hypo2) != 0 else 0,\n",
    "        'hypo_match_mult_fraq_hypo': hypo_match * (len(hypo1) / len(hypo2) if len(hypo2) != 0 else 0),\n",
    "\n",
    "        'hyper_match': hyper_match,\n",
    "        'hyper_match_perc': hyper_match / max(len(hyper1), len(hyper2)) if max(len(hyper1), len(hyper2)) != 0 else 0,\n",
    "        'hyper1_count': len(hyper1),\n",
    "        'hyper2_count': len(hyper2),\n",
    "        'is_hypernym': bool(hyper1.intersection({second_word})),\n",
    "        'fraq_hyper1_hyper2': len(hyper1) / len(hyper2) if len(hyper2) != 0 else 0,\n",
    "        'hyper_match_mult_fraq_hyper': hyper_match * (len(hyper1) / len(hyper2) if len(hyper2) != 0 else 0),\n",
    "\n",
    "        'polysemy1': poly1, \n",
    "        'polysemy2': poly2, \n",
    "\n",
    "        'are_homonyms': any(s1.name().split('.')[0] == s2.name().split('.')[0] for s1 in wn.synsets(first_word) for s2 in wn.synsets(second_word)),\n",
    "        'common_homonyms': len(set(s1.name().split('.')[0] for s1 in wn.synsets(first_word)).intersection(set(s2.name().split('.')[0] for s2 in wordnet.synsets(second_word)))),\n",
    "        'common_homonyms_perc': len(set(s1.name().split('.')[0] for s1 in wn.synsets(first_word)).intersection(set(s2.name().split('.')[0] for s2 in wn.synsets(second_word)))) / max(len(wordnet.synsets(first_word)), len(wordnet.synsets(second_word))) if max(len(wordnet.synsets(first_word)), len(wordnet.synsets(second_word))) != 0 else 0,\n",
    "\n",
    "        'first_word_lemma': lemma1,\n",
    "        'second_word_lemma': lemma2,\n",
    "        'common_lemma': common_lemma,\n",
    "        'common_prefixes': common_prefixes,\n",
    "        'common_suffixes': common_suffixes,\n",
    "        'first_word_derivations': deriv1,\n",
    "        'second_word_derivations': deriv2,\n",
    "        'are_derivationally_related': second_word in deriv1 or first_word in deriv2,\n",
    "\n",
    "        'word1_pos_tags': pos1,\n",
    "        'word2_pos_tags': pos2,\n",
    "        'common_pos_tags': common_pos_tags,\n",
    "        'same_pos_tags': same_pos_tags,\n",
    "\n",
    "        'first_word_dependencies': dep1,\n",
    "        'second_word_dependencies': dep2,\n",
    "        'first_word_dep_types': dep_types1,\n",
    "        'second_word_dep_types': dep_types2,\n",
    "        'first_word_head_words': head_words1,\n",
    "        'second_word_head_words': head_words2,\n",
    "        'first_word_dep_count': dep_count1,\n",
    "        'second_word_dep_count': dep_count2,\n",
    "        'common_dep_types': common_dep_types,\n",
    "        'common_head_words': common_head_words,\n",
    "        'dep_count_diff': dep_count_diff\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_lexical_relationships(df):\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_single_pair, row) for _, row in df.iterrows()]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return pd.concat([df.reset_index(drop=True), result_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c975ccc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>first_second</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>no_words_corpus</th>\n",
       "      <th>frequency_w1</th>\n",
       "      <th>frequency_w2</th>\n",
       "      <th>...</th>\n",
       "      <th>fraq_w1_w2</th>\n",
       "      <th>frequency_of_cooc_w_3</th>\n",
       "      <th>corr_w_3</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_3</th>\n",
       "      <th>frequency_of_cooc_w_5</th>\n",
       "      <th>corr_w_5</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_5</th>\n",
       "      <th>frequency_of_cooc_w_8</th>\n",
       "      <th>corr_w_8</th>\n",
       "      <th>freq_cooc_mult_fraq_w1_w2_w_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>river</td>\n",
       "      <td>water</td>\n",
       "      <td>0.49</td>\n",
       "      <td>river_water</td>\n",
       "      <td>1639782</td>\n",
       "      <td>river</td>\n",
       "      <td>water</td>\n",
       "      <td>509377579</td>\n",
       "      <td>1885</td>\n",
       "      <td>42608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044241</td>\n",
       "      <td>83</td>\n",
       "      <td>0.189442</td>\n",
       "      <td>3.671963</td>\n",
       "      <td>136</td>\n",
       "      <td>0.295877</td>\n",
       "      <td>6.016710</td>\n",
       "      <td>207</td>\n",
       "      <td>0.399837</td>\n",
       "      <td>9.157787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rain</td>\n",
       "      <td>storm</td>\n",
       "      <td>0.49</td>\n",
       "      <td>rain_storm</td>\n",
       "      <td>1122299</td>\n",
       "      <td>rain</td>\n",
       "      <td>storm</td>\n",
       "      <td>509377579</td>\n",
       "      <td>1675</td>\n",
       "      <td>666</td>\n",
       "      <td>...</td>\n",
       "      <td>2.515015</td>\n",
       "      <td>35</td>\n",
       "      <td>0.494082</td>\n",
       "      <td>88.025526</td>\n",
       "      <td>41</td>\n",
       "      <td>0.622359</td>\n",
       "      <td>103.115616</td>\n",
       "      <td>47</td>\n",
       "      <td>0.734905</td>\n",
       "      <td>118.205706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>kittens</td>\n",
       "      <td>0.49</td>\n",
       "      <td>cat_kittens</td>\n",
       "      <td>1348876</td>\n",
       "      <td>cat</td>\n",
       "      <td>kittens</td>\n",
       "      <td>509377579</td>\n",
       "      <td>2711</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>28.840426</td>\n",
       "      <td>6</td>\n",
       "      <td>0.423349</td>\n",
       "      <td>173.042553</td>\n",
       "      <td>11</td>\n",
       "      <td>0.508188</td>\n",
       "      <td>317.244681</td>\n",
       "      <td>15</td>\n",
       "      <td>0.568514</td>\n",
       "      <td>432.606383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat</td>\n",
       "      <td>feline</td>\n",
       "      <td>0.48</td>\n",
       "      <td>cat_feline</td>\n",
       "      <td>1348828</td>\n",
       "      <td>cat</td>\n",
       "      <td>feline</td>\n",
       "      <td>509377579</td>\n",
       "      <td>2711</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>58.934783</td>\n",
       "      <td>2</td>\n",
       "      <td>0.306128</td>\n",
       "      <td>117.869565</td>\n",
       "      <td>7</td>\n",
       "      <td>0.440324</td>\n",
       "      <td>412.543478</td>\n",
       "      <td>10</td>\n",
       "      <td>0.524349</td>\n",
       "      <td>589.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beach</td>\n",
       "      <td>sand</td>\n",
       "      <td>0.48</td>\n",
       "      <td>beach_sand</td>\n",
       "      <td>1475391</td>\n",
       "      <td>beach</td>\n",
       "      <td>sand</td>\n",
       "      <td>509377579</td>\n",
       "      <td>2996</td>\n",
       "      <td>1247</td>\n",
       "      <td>...</td>\n",
       "      <td>2.402566</td>\n",
       "      <td>56</td>\n",
       "      <td>0.403957</td>\n",
       "      <td>134.543705</td>\n",
       "      <td>79</td>\n",
       "      <td>0.551711</td>\n",
       "      <td>189.802727</td>\n",
       "      <td>98</td>\n",
       "      <td>0.662852</td>\n",
       "      <td>235.451484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>cheetah</td>\n",
       "      <td>phone</td>\n",
       "      <td>0.03</td>\n",
       "      <td>phone_cheetah</td>\n",
       "      <td>1649761</td>\n",
       "      <td>phone</td>\n",
       "      <td>cheetah</td>\n",
       "      <td>509377579</td>\n",
       "      <td>24611</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>665.162162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.221936</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>jellyfish</td>\n",
       "      <td>rally</td>\n",
       "      <td>0.02</td>\n",
       "      <td>rally_jellyfish</td>\n",
       "      <td>2284209</td>\n",
       "      <td>rally</td>\n",
       "      <td>jellyfish</td>\n",
       "      <td>509377579</td>\n",
       "      <td>123</td>\n",
       "      <td>132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>military</td>\n",
       "      <td>tomato</td>\n",
       "      <td>0.02</td>\n",
       "      <td>tomato_military</td>\n",
       "      <td>762213</td>\n",
       "      <td>tomato</td>\n",
       "      <td>military</td>\n",
       "      <td>509377579</td>\n",
       "      <td>11952</td>\n",
       "      <td>1046</td>\n",
       "      <td>...</td>\n",
       "      <td>11.426386</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195487</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>festival</td>\n",
       "      <td>whiskers</td>\n",
       "      <td>0.01</td>\n",
       "      <td>whiskers_festival</td>\n",
       "      <td>1139213</td>\n",
       "      <td>whiskers</td>\n",
       "      <td>festival</td>\n",
       "      <td>509377579</td>\n",
       "      <td>18</td>\n",
       "      <td>1734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170930</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>bakery</td>\n",
       "      <td>zebra</td>\n",
       "      <td>0.00</td>\n",
       "      <td>zebra_bakery</td>\n",
       "      <td>411775</td>\n",
       "      <td>zebra</td>\n",
       "      <td>bakery</td>\n",
       "      <td>509377579</td>\n",
       "      <td>71</td>\n",
       "      <td>4976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.201005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2976 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  score       first_second  Unnamed: 0     First  \\\n",
       "0         river     water   0.49        river_water     1639782     river   \n",
       "1          rain     storm   0.49         rain_storm     1122299      rain   \n",
       "2           cat   kittens   0.49        cat_kittens     1348876       cat   \n",
       "3           cat    feline   0.48         cat_feline     1348828       cat   \n",
       "4         beach      sand   0.48         beach_sand     1475391     beach   \n",
       "...         ...       ...    ...                ...         ...       ...   \n",
       "1486    cheetah     phone   0.03      phone_cheetah     1649761     phone   \n",
       "1487  jellyfish     rally   0.02    rally_jellyfish     2284209     rally   \n",
       "1488   military    tomato   0.02    tomato_military      762213    tomato   \n",
       "1489   festival  whiskers   0.01  whiskers_festival     1139213  whiskers   \n",
       "1490     bakery     zebra   0.00       zebra_bakery      411775     zebra   \n",
       "\n",
       "           Last  no_words_corpus  frequency_w1  frequency_w2  ...  fraq_w1_w2  \\\n",
       "0         water        509377579          1885         42608  ...    0.044241   \n",
       "1         storm        509377579          1675           666  ...    2.515015   \n",
       "2       kittens        509377579          2711            94  ...   28.840426   \n",
       "3        feline        509377579          2711            46  ...   58.934783   \n",
       "4          sand        509377579          2996          1247  ...    2.402566   \n",
       "...         ...              ...           ...           ...  ...         ...   \n",
       "1486    cheetah        509377579         24611            37  ...  665.162162   \n",
       "1487  jellyfish        509377579           123           132  ...    0.931818   \n",
       "1488   military        509377579         11952          1046  ...   11.426386   \n",
       "1489   festival        509377579            18          1734  ...    0.010381   \n",
       "1490     bakery        509377579            71          4976  ...    0.014268   \n",
       "\n",
       "      frequency_of_cooc_w_3  corr_w_3  freq_cooc_mult_fraq_w1_w2_w_3  \\\n",
       "0                        83  0.189442                       3.671963   \n",
       "1                        35  0.494082                      88.025526   \n",
       "2                         6  0.423349                     173.042553   \n",
       "3                         2  0.306128                     117.869565   \n",
       "4                        56  0.403957                     134.543705   \n",
       "...                     ...       ...                            ...   \n",
       "1486                      0  0.135797                       0.000000   \n",
       "1487                      0  0.137634                       0.000000   \n",
       "1488                      0  0.060476                       0.000000   \n",
       "1489                      0  0.113956                       0.000000   \n",
       "1490                      0  0.140158                       0.000000   \n",
       "\n",
       "      frequency_of_cooc_w_5  corr_w_5  freq_cooc_mult_fraq_w1_w2_w_5  \\\n",
       "0                       136  0.295877                       6.016710   \n",
       "1                        41  0.622359                     103.115616   \n",
       "2                        11  0.508188                     317.244681   \n",
       "3                         7  0.440324                     412.543478   \n",
       "4                        79  0.551711                     189.802727   \n",
       "...                     ...       ...                            ...   \n",
       "1486                      0  0.181042                       0.000000   \n",
       "1487                      0  0.176777                       0.000000   \n",
       "1488                      0  0.119615                       0.000000   \n",
       "1489                      0  0.134689                       0.000000   \n",
       "1490                      0  0.201005                       0.000000   \n",
       "\n",
       "      frequency_of_cooc_w_8  corr_w_8  freq_cooc_mult_fraq_w1_w2_w_8  \n",
       "0                       207  0.399837                       9.157787  \n",
       "1                        47  0.734905                     118.205706  \n",
       "2                        15  0.568514                     432.606383  \n",
       "3                        10  0.524349                     589.347826  \n",
       "4                        98  0.662852                     235.451484  \n",
       "...                     ...       ...                            ...  \n",
       "1486                      0  0.221936                       0.000000  \n",
       "1487                      0  0.244290                       0.000000  \n",
       "1488                      0  0.195487                       0.000000  \n",
       "1489                      0  0.170930                       0.000000  \n",
       "1490                      0  0.273046                       0.000000  \n",
       "\n",
       "[2976 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed2aa967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2976/2976 [02:51<00:00, 17.32it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_chunk(df_chunk, chunk_number):\n",
    "    res = process_lexical_relationships(df_chunk)\n",
    "    res.to_csv(f\"features_lexical_new_chunk_{chunk_number}.csv\", index=False)\n",
    "    return res\n",
    "\n",
    "chunk_size = int(len(fin) * 1)\n",
    "\n",
    "for i in range(10):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size if (i + 1) * chunk_size <= len(fin) else len(fin)\n",
    "    df_chunk = fin.iloc[start_idx:end_idx]\n",
    "    process_chunk(df_chunk, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
