{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "import gensim\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my own code\n",
    "import prep_data_functions\n",
    "import embeddings_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_length = 3\n",
    "experimental = prep_data_functions.ExperimentalDF(num_documents = 10, document_length=document_length)\n",
    "corpus2_df = experimental.define_first_df()\n",
    "print(corpus2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "docs = list(set(corpus2_df.Document.values))\n",
    "np.random.shuffle(docs)\n",
    "half_len = len(docs) // 2\n",
    "\n",
    "train_docs = docs[:half_len]\n",
    "test_docs = docs[half_len:]\n",
    "\n",
    "words_from_train = corpus2_df[corpus2_df[\"Document\"].isin(train_docs)]\n",
    "df_train = experimental.aggregate_documents(words_from_train)\n",
    "df_train = experimental.tokenize_data('document', df_train)\n",
    "\n",
    "words_from_test = corpus2_df[corpus2_df[\"Document\"].isin(test_docs)]\n",
    "df_test = experimental.aggregate_documents(words_from_test)\n",
    "df_test = experimental.tokenize_data('document', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(corpus2_df)\n",
    "combinations_list = list(product(df1['Word'], repeat=2))\n",
    "df2 = pd.DataFrame(combinations_list, columns=['Word1', 'Word2'])\n",
    "df2['Document1'] = df2['Word1'].map(df1.set_index('Word')['Document'])\n",
    "df2['Document2'] = df2['Word2'].map(df1.set_index('Word')['Document'])\n",
    "df2 = df2[df2[\"Document1\"]!=df2[\"Document2\"]]\n",
    "df2 = df2[[\"Word1\",\"Word2\"]]\n",
    "df2.columns = [\"First\",\"Last\"]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs_train = pd.DataFrame(df_train, columns=['First',\"Middle\", 'Last']).drop_duplicates()\n",
    "unique_pairs_test = pd.DataFrame(df_test, columns=['First', \"Middle\",'Last']).drop_duplicates()\n",
    "    \n",
    "unique_pairs_train[\"join_middle_first\"] = unique_pairs_train[\"First\"]+unique_pairs_train[\"Middle\"]\n",
    "unique_pairs_train[\"join_middle_last\"] = unique_pairs_train[\"Middle\"]+unique_pairs_train[\"Last\"]\n",
    "unique_pairs_train[\"join_first_last\"] = unique_pairs_train[\"First\"]+unique_pairs_train[\"Last\"]\n",
    "unique_pairs_test[\"join_middle_first\"] = unique_pairs_test[\"First\"]+unique_pairs_test[\"Middle\"]\n",
    "unique_pairs_test[\"join_middle_last\"] = unique_pairs_test[\"Middle\"]+unique_pairs_test[\"Last\"]\n",
    "unique_pairs_test[\"join_first_last\"] = unique_pairs_test[\"First\"]+unique_pairs_test[\"Last\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip gram model - low frequency vs high frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_indexes=[int(test_docs[item].split(\"c\")[1]) for item in range(0,len(test_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_list = []\n",
    "avg_test_list = []\n",
    "avg_df2_list = []\n",
    "corpus_size_dupl = []\n",
    "corpus_size = []\n",
    "for cnt_of_dupl in tqdm.tqdm(list(range(1, 30000, 500))):\n",
    "    dict_index_count = {test_docs_indexes[item]: cnt_of_dupl for item in range(0,len(test_docs_indexes))}\n",
    "    corpus_size_dupl.append(cnt_of_dupl)\n",
    "    corpus_pokus_t = experimental.create_experiment(dict_index_count=dict_index_count, df=corpus2_df)\n",
    "    corpus_size.append(len(corpus_pokus_t))\n",
    "    word_embeddings = embeddings_functions.Word2VecEmbeddings(corpus_pokus_t,\"experiment2\")\n",
    "    words = list(np.array(corpus_pokus_t).ravel())\n",
    "    embeddings = word_embeddings.train_and_get_emb(words=words, window=1, min_count=1, seed=1, sg=1, vector_size=100,sample=None,epochs=5,norm=True)\n",
    "    similarity = embeddings_functions.Similarity2(embeddings_df=embeddings)\n",
    "    df_cosine = similarity.get_cosine_similarity_of_all_words()\n",
    "\n",
    "    df_cosine[\"join_middle_first\"] = df_cosine[\"First\"] + df_cosine[\"Last\"]\n",
    "    df_cosine[\"join_middle_last\"] = df_cosine[\"First\"] + df_cosine[\"Last\"]\n",
    "    df_cosine[\"join_first_last\"] = df_cosine[\"First\"] + df_cosine[\"Last\"]\n",
    "    \n",
    "    sim_vals_train = df_cosine.merge(unique_pairs_train, on=['join_middle_first'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_vals_2_train = df_cosine.merge(unique_pairs_train, on=['join_middle_last'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_list_train = sim_vals_train + sim_vals_2_train\n",
    "    \n",
    "    sim_vals_test = df_cosine.merge(unique_pairs_test, on=['join_middle_first'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_vals_2_test = df_cosine.merge(unique_pairs_test, on=['join_middle_last'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_list_test = sim_vals_test + sim_vals_2_test\n",
    "\n",
    "    avg_test = np.average(sim_list_test)\n",
    "    avg_test_list.append(avg_test)\n",
    "    \n",
    "    avg_train = np.average(sim_list_train)\n",
    "    avg_train_list.append(avg_train)\n",
    "    \n",
    "    avg_df2 = np.average(df_cosine.merge(df2, on = [\"First\",\"Last\"])[\"similarity\"].tolist())\n",
    "    avg_df2_list.append(avg_df2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_list2 = []\n",
    "avg_test_list2 = []\n",
    "avg_df2_list2 = []\n",
    "corpus_size_dupl2 = []\n",
    "corpus_size2 = []\n",
    "for cnt_of_dupl in tqdm.tqdm(list(range(1, 30000, 500))):\n",
    "#for cnt_of_dupl in tqdm.tqdm(list(range(1, 10, 2))):\n",
    "    dict_index_count = {test_docs_indexes[item]: cnt_of_dupl for item in range(0,len(test_docs_indexes))}\n",
    "    corpus_size_dupl2.append(cnt_of_dupl)\n",
    "    corpus_pokus_t = experimental.create_experiment(dict_index_count=dict_index_count, df=corpus2_df)\n",
    "    corpus_size2.append(len(corpus_pokus_t))\n",
    "    word_embeddings = embeddings_functions.FastTextEmbeddings(corpus_pokus_t,\"experiment2_ft\")\n",
    "    words = list(np.array(corpus_pokus_t).ravel())\n",
    "    embeddings = word_embeddings.train_and_get_emb(words=words, window=1, min_count=1, seed=1, sg=1, vector_size=100,sample=None,epochs=5,norm=True)\n",
    "    similarity = embeddings_functions.Similarity2(embeddings_df=embeddings)\n",
    "    df_cosine = similarity.get_cosine_similarity_of_all_words()\n",
    "\n",
    "    df_cosine[\"join_middle_first\"] = df_cosine[\"First\"] + df_cosine[\"Last\"]\n",
    "    df_cosine[\"join_middle_last\"] = df_cosine[\"First\"] + df_cosine[\"Last\"]\n",
    "    df_cosine[\"join_first_last\"] = df_cosine[\"First\"] + df_cosine[\"Last\"]\n",
    "    \n",
    "    sim_vals_train = df_cosine.merge(unique_pairs_train, on=['join_middle_first'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_vals_2_train = df_cosine.merge(unique_pairs_train, on=['join_middle_last'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_list_train = sim_vals_train + sim_vals_2_train\n",
    "    \n",
    "    sim_vals_test = df_cosine.merge(unique_pairs_test, on=['join_middle_first'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_vals_2_test = df_cosine.merge(unique_pairs_test, on=['join_middle_last'], how='inner')[\"similarity\"].tolist()\n",
    "    sim_list_test = sim_vals_test + sim_vals_2_test\n",
    "\n",
    "    avg_test = np.average(sim_list_test)\n",
    "    avg_test_list2.append(avg_test)\n",
    "    \n",
    "    avg_train = np.average(sim_list_train)\n",
    "    avg_train_list2.append(avg_train)\n",
    "    \n",
    "    avg_df2 = np.average(df_cosine.merge(df2, on = [\"First\",\"Last\"])[\"similarity\"].tolist())\n",
    "    avg_df2_list2.append(avg_df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(avg_train_list, label='Words once appeared in same context')\n",
    "plt.plot(avg_test_list, label='Words frequently appeared togather')\n",
    "plt.plot(avg_df2_list, label='Words never appeared togather')\n",
    "plt.legend()  \n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(avg_train_list2, label='Words once appeared in same context')\n",
    "plt.plot(avg_test_list2, label='Words frequently appeared togather')\n",
    "plt.plot(avg_df2_list2, label='Words never appeared togather')\n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine[\"similarity\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine.sort_values(\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine.sort_values(\"similarity\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
